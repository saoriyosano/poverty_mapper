{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet V3 IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/africa_poverty_clean/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from batchers import dataset_constants, tfrecord_paths_utils\n",
    "from models import processing\n",
    "from models.loss import r2\n",
    "from models.checkpoint import CustomModelCheckpoint\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST_BEST_WEIGHT = 'models/checkpoints/MN_RGB_20230307-073729/model_epoch02.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DHS_TFRECORDS_PATH_ROOT = 'data/dhs_tfrecords/' #local\n",
    "DHS_TFRECORDS_PATH_ROOT = 'gcs/dhs_tfrecords/' #VM\n",
    "CSV_PATH = 'data/dhs_clusters.csv' \n",
    "CHECKPOINT_PATH = 'models/checkpoints/' \n",
    "\n",
    "CLUSTERS_DF = pd.read_csv(CSV_PATH, float_precision='high', index_col=False)\n",
    "IR_BANDS = ['NIR', 'SWIR1', 'SWIR2']\n",
    "RGB_BANDS = ['RED', 'GREEN', 'BLUE']\n",
    "MEANS = dataset_constants._MEANS_DHS\n",
    "STDS = dataset_constants._STD_DEVS_DHS\n",
    "BATCH_SIZE = 8\n",
    "DATASET = 'DHS_OOC_A'\n",
    "SHUFFLE = 16\n",
    "PREFETCH = 2\n",
    "EPOCHS = 100 # CHANGE\n",
    "STEPS_PER_EPOCH = 1474\n",
    "VALIDATION_STEPS = 488 #has to be <= (size of val_ds / batch_size)  \n",
    "\n",
    "### NOTE: (size of dataset / batch size) has to be >= steps_per_epoch * epochs!!!!!\n",
    "### DHS_OOC_A's train, val, test =  11,797, 3,909, 3,963\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datasets(dataset, shuffle, batch_size, prefetch, epochs):\n",
    "    ''' prepares train_ds, val_ds and test_ds'''\n",
    "    \n",
    "    train_tfrecord_paths = tfrecord_paths_utils.dhs_ooc(dataset, split=\"train\")\n",
    "    val_tfrecord_paths = tfrecord_paths_utils.dhs_ooc(dataset, split=\"val\")\n",
    "    test_tfrecord_paths = tfrecord_paths_utils.dhs_ooc(dataset, split=\"test\")\n",
    "    \n",
    "    # for testing - comment out\n",
    "#     train_tfrecord_paths = train_tfrecord_paths[:300]\n",
    "#     val_tfrecord_paths = val_tfrecord_paths[300:400]\n",
    "#     test_tfrecord_paths = test_tfrecord_paths[0:1]\n",
    "    print(len(train_tfrecord_paths), len(val_tfrecord_paths), len(test_tfrecord_paths))\n",
    "\n",
    "    train_ds = tf.data.TFRecordDataset(train_tfrecord_paths, compression_type=\"GZIP\")\n",
    "    val_ds = tf.data.TFRecordDataset(val_tfrecord_paths, compression_type=\"GZIP\")\n",
    "    test_ds = tf.data.TFRecordDataset(test_tfrecord_paths, compression_type=\"GZIP\")\n",
    "\n",
    "    # normalize and resize\n",
    "    train_ds = train_ds.map(processing.process_tfrecords_rgb)\n",
    "    val_ds = val_ds.map(processing.process_tfrecords_rgb)  \n",
    "    test_ds = test_ds.map(processing.process_tfrecords_rgb)\n",
    "\n",
    "    # train_ds = train_ds.map(processing.augment)\n",
    "    # val_ds = val_ds.map(processing.augment)  \n",
    "    # test_ds = test_ds.map(processing.augment)\n",
    "\n",
    "    train_ds = train_ds.cache()\n",
    "    train_ds = train_ds.shuffle(shuffle)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = train_ds.repeat(epochs) # repeats the dataset for the number of epochs \n",
    "    train_ds = train_ds.prefetch(prefetch)\n",
    "\n",
    "    val_ds = val_ds.cache()\n",
    "    val_ds = val_ds.shuffle(shuffle)\n",
    "    val_ds = val_ds.batch(batch_size)\n",
    "    val_ds = val_ds.prefetch(prefetch)\n",
    "\n",
    "    test_ds = test_ds.cache()\n",
    "#     test_ds = test_ds.shuffle(shuffle)\n",
    "    test_ds = test_ds.batch(batch_size)\n",
    "    test_ds = test_ds.prefetch(prefetch)\n",
    "\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def simple_model_instantiate():\n",
    "#     input_tensor = tf.keras.Input(shape=(224, 224, 3), name = 'images')\n",
    "#     x = tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu', name='conv_layer_1')(input_tensor)\n",
    "#     x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), name='maxpool_1')(x)\n",
    "#     x = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu', name='conv_layer_2')(x)\n",
    "#     x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), name='maxpool_2')(x)\n",
    "#     x = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu', name='conv_layer_3')(x)\n",
    "#     x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), name='maxpool_3')(x)\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "#     predictions = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "#     model = tf.keras.Model(inputs=input_tensor, outputs=predictions)\n",
    "#     adam = tf.keras.optimizers.Adam(\n",
    "#         learning_rate=0.001,\n",
    "#     )\n",
    "#     model.compile(loss='mse', optimizer=adam, metrics=['mse', 'mae', r2])\n",
    "    \n",
    "#     print(model.summary())\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def EN_instantiate():\n",
    "#     ''' initiates EfficientNetB0 model and prints model summary '''\n",
    "\n",
    "#     input_tensor = tf.keras.Input(shape=(224, 224, 3), name = 'images')\n",
    "#     EN_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top = False, input_tensor = input_tensor)    \n",
    "#     x = EN_model.output\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "#     predictions = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "#     model = tf.keras.Model(inputs=EN_model.input, outputs=predictions)\n",
    "    \n",
    "#     adam = tf.keras.optimizers.Adam(\n",
    "#         learning_rate=0.0001,\n",
    "#     )\n",
    "#     model.compile(loss='mse', optimizer=adam, metrics=['mse', 'mae', r2])\n",
    "#     print(model.summary())\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_instantiate(last_best_weight = None, dropout_rate=None, decay=None):\n",
    "    ''' initiates mobilenet model and prints model summary '''\n",
    "\n",
    "    input_tensor = tf.keras.Input(shape=(224, 224, 3), name = 'images')\n",
    "    mn_model = tf.keras.applications.MobileNetV3Small(\n",
    "        include_top = False, \n",
    "        input_tensor = input_tensor,\n",
    "        dropout_rate = dropout_rate\n",
    "    )    \n",
    "    x = mn_model.output\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    predictions = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    model = tf.keras.Model(inputs=mn_model.input, outputs=predictions)\n",
    "    \n",
    "    if last_best_weight:\n",
    "        model.load_weights(last_best_weight)\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0001,\n",
    "        decay=decay\n",
    "    )\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['mse', 'mae', r2])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, train_ds, val_ds, checkpoint_path, epochs, steps_per_epoch, validation_steps):\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    checkpoint_path = os.path.join(checkpoint_path, f\"MN_RGB_{date_time}\")\n",
    "    \n",
    "    # creates a folder inside models/checkpoints for checkpoints and csv to be saved\n",
    "    # folder name: date_time \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "        \n",
    "    # saves checkpoint at the end of every epoch if val_loss has improved vs the previous epoch \n",
    "    print(f\"checkpoint_path: {checkpoint_path}\")\n",
    "    cp_callback = CustomModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # logs the outcome of every epoch in a csv file\n",
    "    csvpath = os.path.join(checkpoint_path, 'model_history_log.csv')\n",
    "    print(f\"csvpath: {csvpath}\")\n",
    "    with open(csvpath, 'a', encoding='utf-8') as f:\n",
    "        csv_logger = tf.keras.callbacks.CSVLogger(\n",
    "            csvpath,\n",
    "            separator=',',\n",
    "            append=True\n",
    "        )\n",
    "    \n",
    "    hist = model.fit(\n",
    "        train_ds, epochs=epochs, validation_data=val_ds,\n",
    "        callbacks=[cp_callback, csv_logger], verbose=1, \n",
    "        steps_per_epoch=steps_per_epoch, validation_steps=validation_steps\n",
    "    )\n",
    "    \n",
    "    return trained_model, hist.history, hist.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = process_datasets(DATASET, SHUFFLE, BATCH_SIZE, PREFETCH, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in train_ds.take(1):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = mobilenet_instantiate(last_best_weight=LAST_BEST_WEIGHT, dropout_rate=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_path: models/checkpoints/MN_RGB_20230307-073729\n",
      "csvpath: models/checkpoints/MN_RGB_20230307-073729/model_history_log.csv\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f1419bc9ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f1419bc9ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function r2 at 0x7f1419bc38c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function r2 at 0x7f1419bc38c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1317/1474 [=========================>....] - ETA: 14:54 - loss: 0.0170 - mse: 0.0170 - mae: 0.0983 - r2: 0.8847"
     ]
    }
   ],
   "source": [
    "trained_model, history, params = train_val(model, train_ds, val_ds, CHECKPOINT_PATH, EPOCHS, STEPS_PER_EPOCH, VALIDATION_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "common-cu100.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu100:m84"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
